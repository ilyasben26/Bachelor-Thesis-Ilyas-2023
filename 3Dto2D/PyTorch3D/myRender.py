import os
import sys
import torch
import numpy as np
import json
from owlready2 import *
import clip
import PIL
from PIL import Image

from transformers import CLIPProcessor, CLIPModel

import matplotlib.pyplot as plt
from skimage.io import imread
from utils import *

from pytorch3d.io import load_obj
from pytorch3d.structures import Meshes
from pytorch3d.renderer import (
    look_at_view_transform,
    FoVPerspectiveCameras, 
    FoVOrthographicCameras, 
    Materials, 
    RasterizationSettings, 
    MeshRenderer, 
    MeshRasterizer,  
    SoftPhongShader,
    TexturesVertex,
    TexturesAtlas,
    PointsRenderer,
    PointsRasterizationSettings,
    PointsRasterizer
)

## Pytorch3D Render Functions ##

def bounding_sphere_exact_from_obj(obj_filename):
    """
    Computes the bounding sphere of a set of a .obj model.
    
    Args:
    - obj_filename: file path of the .obj file
    
    Returns:
    - center: a numpy array of shape (3,) representing the center of the bounding sphere
    - radius: a float representing the radius of the bounding sphere
    """
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        torch.cuda.set_device(device)
    else:
        device = torch.device("cpu")

    verts, faces, aux = load_obj(
        obj_filename,
        device=device,
        load_textures=True,
        create_texture_atlas=True,
        texture_atlas_size=4,
        texture_wrap="repeat"
     )

    verts=verts.numpy()
    
    # Compute the center of mass
    center = np.mean(verts, axis=0)
    
    # Compute the radius
    radius = np.max(np.linalg.norm(verts - center, axis=1))
    
    return center, radius


def bounding_sphere_exact(points):
    """
    Computes the bounding sphere of a set of points in 3D space.
    
    Args:
    - points: a numpy array of shape (n, 3) representing the n points in 3D space
    
    Returns:
    - center: a numpy array of shape (3,) representing the center of the bounding sphere
    - radius: a float representing the radius of the bounding sphere
    """
    # Compute the center of mass
    center = np.mean(points, axis=0)
    
    # Compute the radius
    radius = np.max(np.linalg.norm(points - center, axis=1))
    
    return center, radius




def myRender(obj_filename, elevation, azim_angle, camera_dist, image_size, batch_size) :
    """
    Renders images from different view-points.
    
    Args:
    - obj_filename: file path of the .obj file to be rendered
    - elevation: camera elevation
    - azim_angle: camera angle or model rotation (default is 0)
    - camera_dist: camera distance from the model
    
    Returns:
    - images: a list of tensor images of shape (batch_size ,image_size, image_size, 4)
    """
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        torch.cuda.set_device(device)
    else:
        device = torch.device("cpu")

    obj_filename = obj_filename

   
    verts, faces, aux = load_obj(
        obj_filename,
        device=device,
        load_textures=True,
        create_texture_atlas=True,
        texture_atlas_size=4,
        texture_wrap="repeat"
     )


    atlas = aux.texture_atlas

    object_mesh = Meshes(
        verts=[verts],
        faces=[faces.verts_idx],
        textures=TexturesAtlas(atlas=[atlas]),)
    
    print('We have {0} vertices and {1} faces.'.format(verts.shape[0], faces.verts_idx.shape[0]))

    #R, T = look_at_view_transform(dist = camera_dist, elev = elevation, azim = azim_angle) 
    #cameras = FoVPerspectiveCameras(device=device, R=R, T=T)
    
    elev = torch.linspace(-360, 360, batch_size)
    azim = torch.linspace(0, 330, batch_size)

    R, T = look_at_view_transform(dist = camera_dist, elev = elevation, azim = azim)

    cameras = FoVPerspectiveCameras(device=device, R=R, T=T)

    raster_settings = RasterizationSettings(
        image_size = image_size, 
        blur_radius = 0.0, 
        faces_per_pixel = 1, 
    )

    rasterizer = MeshRasterizer(
        cameras=cameras, 
        raster_settings=raster_settings
    )

  
    shader = SoftPhongShader(device = device, cameras = cameras)
   
    renderer = MeshRenderer(rasterizer, shader)
 
    meshes = object_mesh.extend(batch_size)

    images = renderer(meshes, cameras=cameras)

    return images

def tensor_to_image(tensor):
    """
    Transform a tensor images generated by the renderer to a PIL image.
    
    Args:
    - tensor: Tensor image of shape (w, h, 3)
    
    Returns:
    - PIL image
    """
    
    tensor = tensor*255
    tensor = np.array(tensor, dtype=np.uint8)
    if np.ndim(tensor)>3:
        assert tensor.shape[0] == 1
        tensor = tensor[0]
    
    return PIL.Image.fromarray(tensor)

def tensor_to_PIL_images(tensors):
    """
    Transforms tensor images generated by the renderer to PIL images.
    
    Args:
    - tensors: list of tensor images of shape (w, h, 4)
    
    Returns:
    - images_PIL: list of PIL images
    """
    images_PIL = []
    for i in range(tensors.shape[0]):
        images_PIL.append(tensor_to_image(tensors[i][..., :3]))
    return images_PIL


## CLIP Functions ##
def PIL_image_to_probs(image, classes_strings):
    """
    runs CLIP on PIL image with given classes
    
    Args:
    - image: a PIL image
    
    Returns:
    - probs: list of probabilities of shape (len(classes_strings))
    """
    
    model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    
    inputs = processor(text=classes_strings, images=image, return_tensors="pt", padding=True)
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image # this is the image-text similarity score
    probs = logits_per_image.softmax(dim=1).cpu().detach().numpy() # we can take the softmax to get the label probabilities
    
    return probs[0].tolist()

## Ontology Functions ##

def fetch_ontology(path):
    return get_ontology("file://"+path).load()


def find_root_classes(ontology):
    # find one class that has owl.Thing as a parent 
    for OntoClass in ontology.classes():
        if(OntoClass.is_a[0].iri == "http://www.w3.org/2002/07/owl#Thing"):
            # then use it to find all the highest level classes
            return list(OntoClass.is_a[0].subclasses())
    
def convert_classes_to_strings(classes):
    output_list = list(map(lambda x: x.name, classes))
    return output_list


def get_children(ontoClass):
    return list(ontoClass.subclasses())

def predict_image_ontology (pil_image, ontology_path, print_flag):
    ontology = fetch_ontology(ontology_path)
    current_ontology_level = find_root_classes(ontology)
    current_ontology_level_strings = convert_classes_to_strings(current_ontology_level)
    prob_list = PIL_image_to_probs(pil_image, current_ontology_level_strings)
    
    if(print_flag):
        print('------------------------')
        print(current_ontology_level_strings)
        print(prob_list)
    
    while True:
        max_index = prob_list.index(max(prob_list))
        
        chosen_class = current_ontology_level[max_index]
        
        if(print_flag):
            print(chosen_class.name + ': ' + str(prob_list[max_index]) + '\n------------------------')
        
        current_ontology_level = get_children(chosen_class)
        
        if current_ontology_level == []:
            if(print_flag):
                print("It's a " + chosen_class.name + '.')
                print('------------------------')
            return (prob_list[max_index], chosen_class.name)
            
        current_ontology_level_strings = convert_classes_to_strings(current_ontology_level)
        prob_list = PIL_image_to_probs(pil_image, current_ontology_level_strings)
        
        if(print_flag):
            print(current_ontology_level_strings)
            print(prob_list)

