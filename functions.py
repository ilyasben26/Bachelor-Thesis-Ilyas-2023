import os
import sys
import torch
import numpy as np
import json
from owlready2 import *
import clip
import PIL
from PIL import Image
import time

from transformers import CLIPProcessor, CLIPModel

from datetime import datetime

import matplotlib.pyplot as plt
from skimage.io import imread
from utils import *

from pytorch3d.io import load_obj
from pytorch3d.structures import Meshes
from pytorch3d.renderer import (
    look_at_view_transform,
    FoVPerspectiveCameras, 
    FoVOrthographicCameras, 
    Materials, 
    RasterizationSettings, 
    MeshRenderer, 
    MeshRasterizer,  
    SoftPhongShader,
    TexturesVertex,
    TexturesAtlas,
    PointsRenderer,
    PointsRasterizationSettings,
    PointsRasterizer
)

## Pytorch3D Render Functions ##

def bounding_sphere_exact_from_obj(obj_filename):
    """
    Computes the bounding sphere of a set of a .obj model.
    
    Args:
    - obj_filename: file path of the .obj file
    
    Returns:
    - center: a numpy array of shape (3,) representing the center of the bounding sphere
    - radius: a float representing the radius of the bounding sphere
    """
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        torch.cuda.set_device(device)
    else:
        device = torch.device("cpu")

    verts, faces, aux = load_obj(
        obj_filename,
        device=device,
        load_textures=True,
        create_texture_atlas=True,
        texture_atlas_size=4,
        texture_wrap="repeat"
     )

    verts=verts.numpy()
    
    # Compute the center
    center = np.mean(verts, axis=0)
    
    # Compute the radius
    radius = np.max(np.linalg.norm(verts - center, axis=1))
    
    return center, radius


def bounding_sphere_exact(points):
    """
    Computes the bounding sphere of a set of points in 3D space.
    
    Args:
    - points: a numpy array of shape (n, 3) representing the n points in 3D space
    
    Returns:
    - center: a numpy array of shape (3,) representing the center of the bounding sphere
    - radius: a float representing the radius of the bounding sphere
    """
    # Compute the center of mass
    center = np.mean(points, axis=0)
    
    # Compute the radius
    radius = np.max(np.linalg.norm(points - center, axis=1))
    
    return center, radius




def myRender(obj_filename, camera_dist, elevation, image_size, batch_size) :
    """
    Renders images from different view-points.
    
    Args:
    - obj_filename: file path of the .obj file to be rendered
    - elevation: camera elevation
    - camera_dist: camera distance from the 3D model
    - image_size: size of the output image in pixels
    - batch_size: number of images to render
    
    Returns:
    - images: a list of tensor images of shape (batch_size ,image_size, image_size, 4)
    """
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        torch.cuda.set_device(device)
    else:
        device = torch.device("cpu")

    obj_filename = obj_filename

    
    # loading the 3D object
    verts, faces, aux = load_obj(
        obj_filename,
        device=device,
        load_textures=True,
        create_texture_atlas=True,
        texture_atlas_size=4,
        texture_wrap="repeat"
     )


    atlas = aux.texture_atlas

    object_mesh = Meshes(
        verts=[verts],
        faces=[faces.verts_idx],
        textures=TexturesAtlas(atlas=[atlas]),)
    
    print('We have {0} vertices and {1} faces.'.format(verts.shape[0], faces.verts_idx.shape[0]))
    
    # defining the cameras
    #elev = torch.linspace(-360, 360, batch_size)
    azim = torch.linspace(0, 330, batch_size)

    R, T = look_at_view_transform(dist = camera_dist, elev = elevation, azim = azim)

    cameras = FoVPerspectiveCameras(device=device, R=R, T=T)
    
    # defining the rasterizer
    raster_settings = RasterizationSettings(
        image_size = image_size, 
        blur_radius = 0.0, 
        faces_per_pixel = 1, 
    )

    rasterizer = MeshRasterizer(
        cameras=cameras, 
        raster_settings=raster_settings
    )
    
  
    # defining the shader
    shader = SoftPhongShader(device = device, cameras = cameras)
    
    # defining the renderer
    renderer = MeshRenderer(rasterizer, shader)
     
    # batch rendering
    meshes = object_mesh.extend(batch_size)

    images = renderer(meshes, cameras=cameras)

    return images

def tensor_to_image(tensor):
    """
    Transform a tensor images generated by the renderer to a PIL image.
    
    Args:
    - tensor: Tensor image of shape (w, h, 3)
    
    Returns:
    - PIL image
    """
    
    tensor = tensor*255
    tensor = np.array(tensor, dtype=np.uint8)
    if np.ndim(tensor)>3:
        assert tensor.shape[0] == 1
        tensor = tensor[0]
    
    return PIL.Image.fromarray(tensor)

def tensor_to_PIL_images(tensors):
    """
    Transforms tensor images generated by the renderer to PIL images.
    
    Args:
    - tensors: list of tensor images of shape (w, h, 4)
    
    Returns:
    - images_PIL: list of PIL images
    """
    images_PIL = []
    for i in range(tensors.shape[0]):
        images_PIL.append(tensor_to_image(tensors[i][..., :3]))
    return images_PIL

def batch_render(obj_filename,image_size, batch_size):
    total_time = 0
    start = time.time()
    center, radius = bounding_sphere_exact_from_obj(obj_filename)
    print(f"Center: {center}")
    print(f"Radius: {radius}")
    end = time.time()
    total_time += end - start
    print('--------------------------')
    print('Bounding Sphere duration: {} seconds'.format(end - start))
    print('--------------------------')
    
    start = time.time()

    images = myRender(
        obj_filename=obj_filename,
        camera_dist=radius*2.5, # distance of the camera from the object or scale of the object (2 -> 5, radius*2.5)
        elevation=30, # elevation is the elevation of the camera 
        #azim_angle=0, # Azimuth Angle is the rotation of the object (it doesn't need to be set)
        image_size=image_size, # size of image in pixels
        batch_size=batch_size # number of pictures to take
    )
    
    # convert result of render to PIL
    images_read = tensor_to_PIL_images(images)
    
    end = time.time()
    total_time += end - start
    print('--------------------------')
    print('Rendering Images duration: {} seconds'.format(end - start))
    print('--------------------------')
    
    # Plot the rendered images
    image_grid(images.cpu().numpy(), cols=3, rows=4, rgb=True)
    
    print('--------------------------')
    print('Total Batch_: {} seconds'.format(total_time))
    print('--------------------------')
    return images_read


## CLIP Functions ##
def PIL_image_to_probs(image, classes_strings):
    """
    runs CLIP on PIL image with given classes
    
    Args:
    - image: a PIL image
    
    Returns:
    - probs: list of probabilities of shape (len(classes_strings))
    """
    
    model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    
    inputs = processor(text=classes_strings, images=image, return_tensors="pt", padding=True)
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image # this is the image-text similarity score
    probs = logits_per_image.softmax(dim=1).cpu().detach().numpy() # we can take the softmax to get the label probabilities
    
    return probs[0].tolist()

## Ontology Functions ##

def fetch_ontology(path):
    return get_ontology("file://"+path).load()


def find_root_classes(ontology):
    """
    Fetches all the subclasses of "owl#Thing"
    
    Args:
    - ontology
    
    Returns:
    - list of classes
    """
    # find one class that has owl.Thing as a parent 
    for OntoClass in ontology.classes():
        if(OntoClass.is_a[0].iri == "http://www.w3.org/2002/07/owl#Thing"):
            # then use it to find all the highest level classes
            return list(OntoClass.is_a[0].subclasses())
    
def convert_classes_to_strings(classes):
    """
    converts classes to "a photo of <class_name>"
    
    Args:
    - ontology classes
    
    Returns:
    - list of strings
    """
    output_list = list(map(lambda x: "a photo of a {}".format(x.name), classes))
    return output_list


def get_children(ontoClass):
    """
    gets a list of subclasses for a given class
    
    Args:
    - ontology class
    
    Returns:
    - list of classes
    """
    return list(ontoClass.subclasses())

def predict_image_ontology (pil_image, ontology_path,  print_flag, starting_class=None):
    """
    uses CLIP to predict the content of an image by traversing the ontology
    
    Args:
    - pil_image: image to classify
    - ontology_path: path to ontology
    - print_flag: bool
    - starting_class: class to consider as root
    
    Returns:
    - tuple of the predicted class and its probability
    """
    ontology = fetch_ontology(ontology_path)
    
    if starting_class is None:  
        current_ontology_level = find_root_classes(ontology)
    else:
        current_ontology_level = list(ontology[starting_class].subclasses())
        
    current_ontology_level_strings = convert_classes_to_strings(current_ontology_level)  
    prob_list = PIL_image_to_probs(pil_image, current_ontology_level_strings)
    
    if(print_flag):
        print('------------------------')
        print(dict(zip(current_ontology_level_strings, prob_list)))
    
    while True:
        max_index = prob_list.index(max(prob_list))
        
        chosen_class = current_ontology_level[max_index]
        
        if(print_flag):
            print(chosen_class.name + ': ' + str(prob_list[max_index]) + '\n------------------------')
        
        current_ontology_level = get_children(chosen_class)
        
        if current_ontology_level == []:
            if(print_flag):
                print("It's a " + chosen_class.name + '.')
                print('------------------------')
            return (prob_list[max_index], chosen_class.name)
            
        current_ontology_level_strings = convert_classes_to_strings(current_ontology_level)
        prob_list = PIL_image_to_probs(pil_image, current_ontology_level_strings)
        
        if(print_flag):
            print(dict(zip(current_ontology_level_strings, prob_list)))
            
            
def predict_batch_ontology(images,ontology_path,starting_class=None):
    """
    runs predict_image_ontology function on multiple images 
    
    Args:
    - images: list of PIL images
    - ontology_path: path to ontology
    - starting_class: class to consider as root
    
    Returns:
    - tuple of the predicted classes and their probabilities
    """
    predictions_class_list = []
    predictions_value_list = []

    CLIP_start = time.time()
    ontology = fetch_ontology(ontology_path)
    for i in range(len(images)):
        prediction_value, prediction_class = predict_image_ontology (images[i], ontology_path, False, starting_class=starting_class)
        predictions_class_list.append(prediction_class)
        predictions_value_list.append(prediction_value)
        print('-------------------')
        print('image index: {}'.format(i))
        print(prediction_class)
        print(prediction_value)
        print('-------------------')
    CLIP_end = time.time()
    print('--------------------------')
    print('Duration of CLIP predictions for {} images: {} seconds'.format(len(predictions_class_list),CLIP_end - CLIP_start))
    print('--------------------------')
    return predictions_class_list, predictions_value_list

def computeSums(class_list, value_list):
    """
    used on the result of predict_batch_ontology to count the sum of each class' probabilities
    and determine which class will be considered the right one
    
    Args:
    - class_list: list of classes of length n
    - value_list: list of values  of length n
    
    Returns:
    - freq: dictionary
    """
    freq = {}
    for i in range(len(class_list)):
        if (class_list[i] in freq):
            freq[class_list[i]] += value_list[i]
        else:
            freq[class_list[i]] = value_list[i]
    return freq

def predict_Vehicle_data_properties(image, ontology_path, class_name):
    """
    predicts the color property of a vehicle using CLIP
    
    Args:
    - image: PIL image to predict
    - ontology_path
    _ class_name
    
    Returns:
    - D: dictionary containing the results of the CLIP prediction
    """
    ontology = fetch_ontology(ontology_path)
    ontology_class = ontology[class_name]
    # check if the detected class is a subclass of Vehicle
    if ontology.Vehicle in ontology_class.ancestors():
        # if yes then get the color property's comment with all the pxossibilities
        data_properties_list = list(ontology.sample_vehicle.get_properties())
        
        # case by case implementations
        # for color
        colors_list = json.loads(list(ontology.sample_vehicle.get_properties())[0].comment[0])
        # run CLIP with pre-defined prompt syntax
        input_strings = list(map(lambda x: "a photo of a {} {}".format(x, class_name), colors_list))
        #print(input_strings)
        prob_list = PIL_image_to_probs(image, input_strings)
        
        D = dict(zip(colors_list, prob_list))
        return D
    return 0

def createInstance(ontology_path, obj_filename, class_name):
    """
    creates an instance of the given class_name in the given ontology
    and sets some metadata
    
    Args:
    - ontology_path
    - obj_filename: used for metadata
    _ class_name: class to create the instance for
    
    Returns:
    - name of the created instance
    """
    onto = fetch_ontology(ontology_path)
    count = len(list(onto[class_name].instances()))
    instance = onto[class_name]('{}_{}'.format(class_name, count))
    
    json_comment = {
        "creation_date": datetime.now().strftime("%m/%d/%Y, %H:%M:%S"),
        "source_obj": obj_filename
    }
    instance.comment.append(json.dumps(json_comment))
    
    onto.save()
    print('Created instance {} in the ontology'.format(instance))
    return '{}_{}'.format(class_name, count)

def setColorProperty(ontology_path, instance_name, color_name):
    """
    sets the color property of a given instance to the given color name
    
    Args:
    - ontology_path
    - instance_name
    _ color_name
    """
    onto = fetch_ontology(ontology_path)
    onto[instance_name].color.append(max_color)
    onto.save()
    print('Set color to {} for instance {}'.format(max_color, instance_name))
    
def get3Dinfo(obj_filename):
    """
    returns information about the 3D model
    
    Args:
    - obj_filename
    
    Returns:
    - vertices_count
    - faces_count
    """
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        torch.cuda.set_device(device)
    else:
        device = torch.device("cpu")
    # loading the 3D object
    verts, faces, aux = load_obj(
        obj_filename,
        device=device,
        load_textures=True,
        create_texture_atlas=True,
        texture_atlas_size=4,
        texture_wrap="repeat"
     )
    vertices_count = verts.shape[0]
    faces_count = faces.verts_idx.shape[0]
    return vertices_count, faces_count